# Third Homework

## This homework is mainly focused on the following topics:

* **`Gaussian Mixture Model`**

* **`Expectation Maximization`**

* **`Maximum Likelihood Estimation`**

* **`Maximum a Posteriori Estimation`**


## Codes and Report

* In the seventh problem, I have implemented **`Expectation Maximization`** to provide a **`Gaussian Mixture Model`** uitlizing **`numpy`** for **`Image Classification`** on [**Manchester-Chelsea**](https://github.com/ARokni/Machine-Learning/blob/main/Homework/2/Problem%207/nyc_cyclist_counts.csv) dataset. Moreover, I have investigated the appropriate hyperparameter (numbr of **kernels**) uitlizing **`AIC`** and **`BIC`**. The codes are provided in [p_7]() notebook.


* In the eightth problem, I have implemented **Gaussian Mixture Model** code utilizing **`scikit-learn`** on **`Penguin Dataset`** in order to:

    - **`Preprocess`** the data(**`Normalization`**, **`Z-Score`**).
    - Slecet the **best pair of features** for **classification**.
    - Determine the best number of **Kernels** based on **AIC** and **BIC**.

The codes are provided in [P_8]() notebook.

* In the nineth problem, I have implemented **Gaussian Mixture Model** utilizing **numpy** on **`Moons`** (from **Sklearn** library) to estimate the distribution of each class. The codes are provided in [P_9_NaiveBayes](https://github.com/ARokni/Machine-Learning/blob/main/Homework/2/Problem%209/P_9_NaiveBayes.py) notebook.


* The assignment and my report are available in [Files](https://github.com/ARokni/Machine-Learning/tree/main/Homework/2/Files).